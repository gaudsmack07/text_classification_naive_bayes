{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "import os\n",
    "x = []\n",
    "y = []\n",
    "for i in os.listdir('20_newsgroups'):\n",
    "    for doc in os.listdir('20_newsgroups/'+i):\n",
    "        with open('20_newsgroups/'+i+'/'+doc, \"r\") as file:\n",
    "            x.append((doc, file.read()))\n",
    "            y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'among',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'around',\n",
       " 'as',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'at',\n",
       " 'away',\n",
       " 'b',\n",
       " 'back',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'backs',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'beings',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'case',\n",
       " 'cases',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'come',\n",
       " 'could',\n",
       " 'd',\n",
       " 'did',\n",
       " 'differ',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'do',\n",
       " 'does',\n",
       " 'done',\n",
       " 'down',\n",
       " 'down',\n",
       " 'downed',\n",
       " 'downing',\n",
       " 'downs',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'early',\n",
       " 'either',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'f',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'fact',\n",
       " 'facts',\n",
       " 'far',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'find',\n",
       " 'finds',\n",
       " 'first',\n",
       " 'for',\n",
       " 'four',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'further',\n",
       " 'furthered',\n",
       " 'furthering',\n",
       " 'furthers',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goods',\n",
       " 'got',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'group',\n",
       " 'grouped',\n",
       " 'grouping',\n",
       " 'groups',\n",
       " 'h',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'herself',\n",
       " 'high',\n",
       " 'high',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'i',\n",
       " 'if',\n",
       " 'important',\n",
       " 'in',\n",
       " 'interest',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interests',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kind',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'large',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'least',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'longest',\n",
       " 'm',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'member',\n",
       " 'members',\n",
       " 'men',\n",
       " 'might',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needing',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'new',\n",
       " 'newer',\n",
       " 'newest',\n",
       " 'next',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'noone',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'old',\n",
       " 'older',\n",
       " 'oldest',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'open',\n",
       " 'opened',\n",
       " 'opening',\n",
       " 'opens',\n",
       " 'or',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'ordering',\n",
       " 'orders',\n",
       " 'other',\n",
       " 'others',\n",
       " 'our',\n",
       " 'out',\n",
       " 'over',\n",
       " 'p',\n",
       " 'part',\n",
       " 'parted',\n",
       " 'parting',\n",
       " 'parts',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'places',\n",
       " 'point',\n",
       " 'pointed',\n",
       " 'pointing',\n",
       " 'points',\n",
       " 'possible',\n",
       " 'present',\n",
       " 'presented',\n",
       " 'presenting',\n",
       " 'presents',\n",
       " 'problem',\n",
       " 'problems',\n",
       " 'put',\n",
       " 'puts',\n",
       " 'q',\n",
       " 'quite',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'really',\n",
       " 'right',\n",
       " 'right',\n",
       " 'room',\n",
       " 'rooms',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'says',\n",
       " 'second',\n",
       " 'seconds',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'sees',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showing',\n",
       " 'shows',\n",
       " 'side',\n",
       " 'sides',\n",
       " 'since',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'smallest',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'somewhere',\n",
       " 'state',\n",
       " 'states',\n",
       " 'still',\n",
       " 'still',\n",
       " 'such',\n",
       " 'sure',\n",
       " 't',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'therefore',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinks',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'thoughts',\n",
       " 'three',\n",
       " 'through',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'today',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'turning',\n",
       " 'turns',\n",
       " 'two',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'uses',\n",
       " 'v',\n",
       " 'very',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wants',\n",
       " 'was',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'we',\n",
       " 'well',\n",
       " 'wells',\n",
       " 'went',\n",
       " 'were',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whole',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'would',\n",
       " 'x',\n",
       " 'y',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'z']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''a\n",
    "\n",
    "about\n",
    "\n",
    "above\n",
    "\n",
    "across\n",
    "\n",
    "after\n",
    "\n",
    "again\n",
    "\n",
    "against\n",
    "\n",
    "all\n",
    "\n",
    "almost\n",
    "\n",
    "alone\n",
    "\n",
    "along\n",
    "\n",
    "already\n",
    "\n",
    "also\n",
    "\n",
    "although\n",
    "\n",
    "always\n",
    "\n",
    "among\n",
    "\n",
    "an\n",
    "\n",
    "and\n",
    "\n",
    "another\n",
    "\n",
    "any\n",
    "\n",
    "anybody\n",
    "\n",
    "anyone\n",
    "\n",
    "anything\n",
    "\n",
    "anywhere\n",
    "\n",
    "are\n",
    "\n",
    "area\n",
    "\n",
    "areas\n",
    "\n",
    "around\n",
    "\n",
    "as\n",
    "\n",
    "ask\n",
    "\n",
    "asked\n",
    "\n",
    "asking\n",
    "\n",
    "asks\n",
    "\n",
    "at\n",
    "\n",
    "away\n",
    "\n",
    "b\n",
    "\n",
    "back\n",
    "\n",
    "backed\n",
    "\n",
    "backing\n",
    "\n",
    "backs\n",
    "\n",
    "be\n",
    "\n",
    "became\n",
    "\n",
    "because\n",
    "\n",
    "become\n",
    "\n",
    "becomes\n",
    "\n",
    "been\n",
    "\n",
    "before\n",
    "\n",
    "began\n",
    "\n",
    "behind\n",
    "\n",
    "being\n",
    "\n",
    "beings\n",
    "\n",
    "best\n",
    "\n",
    "better\n",
    "\n",
    "between\n",
    "\n",
    "big\n",
    "\n",
    "both\n",
    "\n",
    "but\n",
    "\n",
    "by\n",
    "\n",
    "c\n",
    "\n",
    "came\n",
    "\n",
    "can\n",
    "\n",
    "cannot\n",
    "\n",
    "case\n",
    "\n",
    "cases\n",
    "\n",
    "certain\n",
    "\n",
    "certainly\n",
    "\n",
    "clear\n",
    "\n",
    "clearly\n",
    "\n",
    "come\n",
    "\n",
    "could\n",
    "\n",
    "d\n",
    "\n",
    "did\n",
    "\n",
    "differ\n",
    "\n",
    "different\n",
    "\n",
    "differently\n",
    "\n",
    "do\n",
    "\n",
    "does\n",
    "\n",
    "done\n",
    "\n",
    "down\n",
    "\n",
    "down\n",
    "\n",
    "downed\n",
    "\n",
    "downing\n",
    "\n",
    "downs\n",
    "\n",
    "during\n",
    "\n",
    "e\n",
    "\n",
    "each\n",
    "\n",
    "early\n",
    "\n",
    "either\n",
    "\n",
    "end\n",
    "\n",
    "ended\n",
    "\n",
    "ending\n",
    "\n",
    "ends\n",
    "\n",
    "enough\n",
    "\n",
    "even\n",
    "\n",
    "evenly\n",
    "\n",
    "ever\n",
    "\n",
    "every\n",
    "\n",
    "everybody\n",
    "\n",
    "everyone\n",
    "\n",
    "everything\n",
    "\n",
    "everywhere\n",
    "\n",
    "f\n",
    "\n",
    "face\n",
    "\n",
    "faces\n",
    "\n",
    "fact\n",
    "\n",
    "facts\n",
    "\n",
    "far\n",
    "\n",
    "felt\n",
    "\n",
    "few\n",
    "\n",
    "find\n",
    "\n",
    "finds\n",
    "\n",
    "first\n",
    "\n",
    "for\n",
    "\n",
    "four\n",
    "\n",
    "from\n",
    "\n",
    "full\n",
    "\n",
    "fully\n",
    "\n",
    "further\n",
    "\n",
    "furthered\n",
    "\n",
    "furthering\n",
    "\n",
    "furthers\n",
    "\n",
    "g\n",
    "\n",
    "gave\n",
    "\n",
    "general\n",
    "\n",
    "generally\n",
    "\n",
    "get\n",
    "\n",
    "gets\n",
    "\n",
    "give\n",
    "\n",
    "given\n",
    "\n",
    "gives\n",
    "\n",
    "go\n",
    "\n",
    "going\n",
    "\n",
    "good\n",
    "\n",
    "goods\n",
    "\n",
    "got\n",
    "\n",
    "great\n",
    "\n",
    "greater\n",
    "\n",
    "greatest\n",
    "\n",
    "group\n",
    "\n",
    "grouped\n",
    "\n",
    "grouping\n",
    "\n",
    "groups\n",
    "\n",
    "h\n",
    "\n",
    "had\n",
    "\n",
    "has\n",
    "\n",
    "have\n",
    "\n",
    "having\n",
    "\n",
    "he\n",
    "\n",
    "her\n",
    "\n",
    "here\n",
    "\n",
    "herself\n",
    "\n",
    "high\n",
    "\n",
    "high\n",
    "\n",
    "high\n",
    "\n",
    "higher\n",
    "\n",
    "highest\n",
    "\n",
    "him\n",
    "\n",
    "himself\n",
    "\n",
    "his\n",
    "\n",
    "how\n",
    "\n",
    "however\n",
    "\n",
    "i\n",
    "\n",
    "if\n",
    "\n",
    "important\n",
    "\n",
    "in\n",
    "\n",
    "interest\n",
    "\n",
    "interested\n",
    "\n",
    "interesting\n",
    "\n",
    "interests\n",
    "\n",
    "into\n",
    "\n",
    "is\n",
    "\n",
    "it\n",
    "\n",
    "its\n",
    "\n",
    "itself\n",
    "\n",
    "j\n",
    "\n",
    "just\n",
    "\n",
    "k\n",
    "\n",
    "keep\n",
    "\n",
    "keeps\n",
    "\n",
    "kind\n",
    "\n",
    "knew\n",
    "\n",
    "know\n",
    "\n",
    "known\n",
    "\n",
    "knows\n",
    "\n",
    "l\n",
    "\n",
    "large\n",
    "\n",
    "largely\n",
    "\n",
    "last\n",
    "\n",
    "later\n",
    "\n",
    "latest\n",
    "\n",
    "least\n",
    "\n",
    "less\n",
    "\n",
    "let\n",
    "\n",
    "lets\n",
    "\n",
    "like\n",
    "\n",
    "likely\n",
    "\n",
    "long\n",
    "\n",
    "longer\n",
    "\n",
    "longest\n",
    "\n",
    "m\n",
    "\n",
    "made\n",
    "\n",
    "make\n",
    "\n",
    "making\n",
    "\n",
    "man\n",
    "\n",
    "many\n",
    "\n",
    "may\n",
    "\n",
    "me\n",
    "\n",
    "member\n",
    "\n",
    "members\n",
    "\n",
    "men\n",
    "\n",
    "might\n",
    "\n",
    "more\n",
    "\n",
    "most\n",
    "\n",
    "mostly\n",
    "\n",
    "mr\n",
    "\n",
    "mrs\n",
    "\n",
    "much\n",
    "\n",
    "must\n",
    "\n",
    "my\n",
    "\n",
    "myself\n",
    "\n",
    "n\n",
    "\n",
    "necessary\n",
    "\n",
    "need\n",
    "\n",
    "needed\n",
    "\n",
    "needing\n",
    "\n",
    "needs\n",
    "\n",
    "never\n",
    "\n",
    "new\n",
    "\n",
    "new\n",
    "\n",
    "newer\n",
    "\n",
    "newest\n",
    "\n",
    "next\n",
    "\n",
    "no\n",
    "\n",
    "nobody\n",
    "\n",
    "non\n",
    "\n",
    "noone\n",
    "\n",
    "not\n",
    "\n",
    "nothing\n",
    "\n",
    "now\n",
    "\n",
    "nowhere\n",
    "\n",
    "number\n",
    "\n",
    "numbers\n",
    "\n",
    "o\n",
    "\n",
    "of\n",
    "\n",
    "off\n",
    "\n",
    "often\n",
    "\n",
    "old\n",
    "\n",
    "older\n",
    "\n",
    "oldest\n",
    "\n",
    "on\n",
    "\n",
    "once\n",
    "\n",
    "one\n",
    "\n",
    "only\n",
    "\n",
    "open\n",
    "\n",
    "opened\n",
    "\n",
    "opening\n",
    "\n",
    "opens\n",
    "\n",
    "or\n",
    "\n",
    "order\n",
    "\n",
    "ordered\n",
    "\n",
    "ordering\n",
    "\n",
    "orders\n",
    "\n",
    "other\n",
    "\n",
    "others\n",
    "\n",
    "our\n",
    "\n",
    "out\n",
    "\n",
    "over\n",
    "\n",
    "p\n",
    "\n",
    "part\n",
    "\n",
    "parted\n",
    "\n",
    "parting\n",
    "\n",
    "parts\n",
    "\n",
    "per\n",
    "\n",
    "perhaps\n",
    "\n",
    "place\n",
    "\n",
    "places\n",
    "\n",
    "point\n",
    "\n",
    "pointed\n",
    "\n",
    "pointing\n",
    "\n",
    "points\n",
    "\n",
    "possible\n",
    "\n",
    "present\n",
    "\n",
    "presented\n",
    "\n",
    "presenting\n",
    "\n",
    "presents\n",
    "\n",
    "problem\n",
    "\n",
    "problems\n",
    "\n",
    "put\n",
    "\n",
    "puts\n",
    "\n",
    "q\n",
    "\n",
    "quite\n",
    "\n",
    "r\n",
    "\n",
    "rather\n",
    "\n",
    "really\n",
    "\n",
    "right\n",
    "\n",
    "right\n",
    "\n",
    "room\n",
    "\n",
    "rooms\n",
    "\n",
    "s\n",
    "\n",
    "said\n",
    "\n",
    "same\n",
    "\n",
    "saw\n",
    "\n",
    "say\n",
    "\n",
    "says\n",
    "\n",
    "second\n",
    "\n",
    "seconds\n",
    "\n",
    "see\n",
    "\n",
    "seem\n",
    "\n",
    "seemed\n",
    "\n",
    "seeming\n",
    "\n",
    "seems\n",
    "\n",
    "sees\n",
    "\n",
    "several\n",
    "\n",
    "shall\n",
    "\n",
    "she\n",
    "\n",
    "should\n",
    "\n",
    "show\n",
    "\n",
    "showed\n",
    "\n",
    "showing\n",
    "\n",
    "shows\n",
    "\n",
    "side\n",
    "\n",
    "sides\n",
    "\n",
    "since\n",
    "\n",
    "small\n",
    "\n",
    "smaller\n",
    "\n",
    "smallest\n",
    "\n",
    "so\n",
    "\n",
    "some\n",
    "\n",
    "somebody\n",
    "\n",
    "someone\n",
    "\n",
    "something\n",
    "\n",
    "somewhere\n",
    "\n",
    "state\n",
    "\n",
    "states\n",
    "\n",
    "still\n",
    "\n",
    "still\n",
    "\n",
    "such\n",
    "\n",
    "sure\n",
    "\n",
    "t\n",
    "\n",
    "take\n",
    "\n",
    "taken\n",
    "\n",
    "than\n",
    "\n",
    "that\n",
    "\n",
    "the\n",
    "\n",
    "their\n",
    "\n",
    "them\n",
    "\n",
    "then\n",
    "\n",
    "there\n",
    "\n",
    "therefore\n",
    "\n",
    "these\n",
    "\n",
    "they\n",
    "\n",
    "thing\n",
    "\n",
    "things\n",
    "\n",
    "think\n",
    "\n",
    "thinks\n",
    "\n",
    "this\n",
    "\n",
    "those\n",
    "\n",
    "though\n",
    "\n",
    "thought\n",
    "\n",
    "thoughts\n",
    "\n",
    "three\n",
    "\n",
    "through\n",
    "\n",
    "thus\n",
    "\n",
    "to\n",
    "\n",
    "today\n",
    "\n",
    "together\n",
    "\n",
    "too\n",
    "\n",
    "took\n",
    "\n",
    "toward\n",
    "\n",
    "turn\n",
    "\n",
    "turned\n",
    "\n",
    "turning\n",
    "\n",
    "turns\n",
    "\n",
    "two\n",
    "\n",
    "u\n",
    "\n",
    "under\n",
    "\n",
    "until\n",
    "\n",
    "up\n",
    "\n",
    "upon\n",
    "\n",
    "us\n",
    "\n",
    "use\n",
    "\n",
    "used\n",
    "\n",
    "uses\n",
    "\n",
    "v\n",
    "\n",
    "very\n",
    "\n",
    "w\n",
    "\n",
    "want\n",
    "\n",
    "wanted\n",
    "\n",
    "wanting\n",
    "\n",
    "wants\n",
    "\n",
    "was\n",
    "\n",
    "way\n",
    "\n",
    "ways\n",
    "\n",
    "we\n",
    "\n",
    "well\n",
    "\n",
    "wells\n",
    "\n",
    "went\n",
    "\n",
    "were\n",
    "\n",
    "what\n",
    "\n",
    "when\n",
    "\n",
    "where\n",
    "\n",
    "whether\n",
    "\n",
    "which\n",
    "\n",
    "while\n",
    "\n",
    "who\n",
    "\n",
    "whole\n",
    "\n",
    "whose\n",
    "\n",
    "why\n",
    "\n",
    "will\n",
    "\n",
    "with\n",
    "\n",
    "within\n",
    "\n",
    "without\n",
    "\n",
    "work\n",
    "\n",
    "worked\n",
    "\n",
    "working\n",
    "\n",
    "works\n",
    "\n",
    "would\n",
    "\n",
    "x\n",
    "\n",
    "y\n",
    "\n",
    "year\n",
    "\n",
    "years\n",
    "\n",
    "yet\n",
    "\n",
    "you\n",
    "\n",
    "young\n",
    "\n",
    "younger\n",
    "\n",
    "youngest\n",
    "\n",
    "your\n",
    "\n",
    "yours\n",
    "\n",
    "z'''\n",
    "\n",
    "word_list = s.split('\\n')\n",
    "\n",
    "stopwords = [word_list[i] for i in range(len(word_list)) if i%2 == 0]\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the vocabulary dictionary\n",
    "vocab_dict = dict()\n",
    "for i in range(len(X_train)):\n",
    "    word_list = []\n",
    "    for word in X_train[i][1].split():\n",
    "        word_new  = word.strip(string.punctuation).lower()\n",
    "        if word_new not in stopwords:  \n",
    "            vocab_dict[word_new] = vocab_dict.get(word_new, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the vocabulary dictionary, we choose the top 4200 words that have the maximum frequency\n",
    "from heapq import nlargest \n",
    "max_freq_words = nlargest(4200, vocab_dict, key=vocab_dict.get)\n",
    "features = max_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting training data to word count vector format\n",
    "X_train_dataset = np.zeros((len(X_train),len(features)))\n",
    "for i in range(len(X_train)):\n",
    "    word_list = [ word.strip(string.punctuation).lower() for word in X_train[i][1].split()]\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            X_train_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting test data to word count vector format\n",
    "X_test_dataset = np.zeros((len(X_test),len(features)))\n",
    "for i in range(len(X_test)):\n",
    "    word_list = [ word.strip(string.punctuation).lower() for word in X_test[i][1].split()]\n",
    "    for word in word_list:\n",
    "        if word in features:\n",
    "            X_test_dataset[i][features.index(word)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.9035807161432287\n",
      "Score on testing data: 0.856\n",
      "Classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.74      0.80      0.77       233\n",
      "           comp.graphics       0.78      0.78      0.78       253\n",
      " comp.os.ms-windows.misc       0.85      0.85      0.85       249\n",
      "comp.sys.ibm.pc.hardware       0.80      0.89      0.84       240\n",
      "   comp.sys.mac.hardware       0.84      0.92      0.88       236\n",
      "          comp.windows.x       0.90      0.82      0.86       240\n",
      "            misc.forsale       0.82      0.84      0.83       261\n",
      "               rec.autos       0.88      0.92      0.90       269\n",
      "         rec.motorcycles       0.89      0.98      0.93       284\n",
      "      rec.sport.baseball       0.95      0.94      0.95       248\n",
      "        rec.sport.hockey       0.96      0.95      0.96       231\n",
      "               sci.crypt       0.96      0.92      0.94       233\n",
      "         sci.electronics       0.86      0.86      0.86       244\n",
      "                 sci.med       0.93      0.89      0.91       256\n",
      "               sci.space       0.92      0.93      0.92       246\n",
      "  soc.religion.christian       0.93      0.98      0.96       252\n",
      "      talk.politics.guns       0.74      0.88      0.81       249\n",
      "   talk.politics.mideast       0.93      0.86      0.89       281\n",
      "      talk.politics.misc       0.73      0.63      0.67       259\n",
      "      talk.religion.misc       0.68      0.46      0.55       236\n",
      "\n",
      "                accuracy                           0.86      5000\n",
      "               macro avg       0.85      0.85      0.85      5000\n",
      "            weighted avg       0.85      0.86      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn's inbuilt function\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_dataset,Y_train)\n",
    "Y_test_pred = clf.predict(X_test_dataset)\n",
    "sklearn_score_train = clf.score(X_train_dataset,Y_train)\n",
    "print(\"Score on training data:\",sklearn_score_train)\n",
    "sklearn_score_test = clf.score(X_test_dataset,Y_test)\n",
    "print(\"Score on testing data:\",sklearn_score_test)\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #count is the main dictionary containing information about the number of occurences of each word in each class \n",
    "        self.count = {}\n",
    "        #classes are the various newsgroups\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self,X_train,Y_train):    \n",
    "        self.classes = set(Y_train)\n",
    "        for class_ in self.classes:\n",
    "            self.count[class_] = {}\n",
    "            for i in range(len(X_train[0])):\n",
    "                self.count[class_][i] = 0\n",
    "            self.count[class_]['total'] = 0\n",
    "            self.count[class_]['total_points'] = 0\n",
    "        self.count['total_points'] = len(X_train)\n",
    "        \n",
    "        for i in range(len(X_train)):\n",
    "            for j in range(len(X_train[0])):\n",
    "                self.count[Y_train[i]][j] += X_train[i][j]\n",
    "                self.count[Y_train[i]]['total'] += X_train[i][j]\n",
    "            self.count[Y_train[i]]['total_points'] += 1\n",
    "    \n",
    "    def __probability(self,test_point,class_):\n",
    "    #This function calculates the probability of the current class, ie the probability of the current document belonging to the current class   \n",
    "        log_prob = np.log(self.count[class_]['total_points']) - np.log(self.count['total_points'])\n",
    "        total_words = len(test_point)\n",
    "        for i in range(len(test_point)):\n",
    "            current_word_prob = test_point[i]*(np.log(self.count[class_][i]+1)-np.log(self.count[class_]['total']+total_words))\n",
    "            log_prob += current_word_prob\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    \n",
    "    def __predictSinglePoint(self,test_point):\n",
    "    #This function chooses the best class probability, ie it choose the class having the maximum probability\n",
    "        best_class = None\n",
    "        best_prob = None\n",
    "        first_run = True\n",
    "        \n",
    "        for class_ in self.classes:\n",
    "            log_probability_current_class = self.__probability(test_point,class_)\n",
    "            if (first_run) or (log_probability_current_class > best_prob) :\n",
    "                best_class = class_\n",
    "                best_prob = log_probability_current_class\n",
    "                first_run = False\n",
    "                \n",
    "        return best_class\n",
    "        \n",
    "  \n",
    "    def predict(self,X_test):\n",
    "        Y_pred = [] \n",
    "        for i in range(len(X_test)):\n",
    "            Y_pred.append( self.__predictSinglePoint(X_test[i]) )\n",
    "        \n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = NaiveBayes()\n",
    "clf2.fit(X_train_dataset,Y_train)\n",
    "Y_test_pred = clf2.predict(X_test_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.74      0.80      0.77       233\n",
      "           comp.graphics       0.78      0.78      0.78       253\n",
      " comp.os.ms-windows.misc       0.85      0.85      0.85       249\n",
      "comp.sys.ibm.pc.hardware       0.80      0.89      0.84       240\n",
      "   comp.sys.mac.hardware       0.84      0.92      0.88       236\n",
      "          comp.windows.x       0.90      0.82      0.86       240\n",
      "            misc.forsale       0.82      0.84      0.83       261\n",
      "               rec.autos       0.88      0.92      0.90       269\n",
      "         rec.motorcycles       0.89      0.98      0.93       284\n",
      "      rec.sport.baseball       0.95      0.94      0.95       248\n",
      "        rec.sport.hockey       0.96      0.95      0.96       231\n",
      "               sci.crypt       0.96      0.92      0.94       233\n",
      "         sci.electronics       0.86      0.86      0.86       244\n",
      "                 sci.med       0.93      0.89      0.91       256\n",
      "               sci.space       0.92      0.93      0.92       246\n",
      "  soc.religion.christian       0.93      0.98      0.96       252\n",
      "      talk.politics.guns       0.74      0.88      0.81       249\n",
      "   talk.politics.mideast       0.93      0.86      0.89       281\n",
      "      talk.politics.misc       0.73      0.63      0.67       259\n",
      "      talk.religion.misc       0.68      0.46      0.55       236\n",
      "\n",
      "                accuracy                           0.86      5000\n",
      "               macro avg       0.85      0.85      0.85      5000\n",
      "            weighted avg       0.85      0.86      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\")\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
